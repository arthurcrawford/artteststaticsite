{
    "docs": [
        {
            "location": "/index.html", 
            "text": "The Zonza Platform\n\n\nThis is the technical documentation starting point for the Zonza platform and its brethren.\n\n\nAudience\n\n\nThe intended audience is anyone involved in the delivery and maintenance of this family of platforms.\n\n\nDifferences from Wiki\n\n\nThis documentation reflects the state of play today.  If it doesn't its a bug.  This documentation is subject to the same release lifecycle as the software systems it describes.  Unlike Wiki usage, there is no attempt to capture history, collaborative input or discuss future possibilities.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#the-zonza-platform", 
            "text": "This is the technical documentation starting point for the Zonza platform and its brethren.", 
            "title": "The Zonza Platform"
        }, 
        {
            "location": "/index.html#audience", 
            "text": "The intended audience is anyone involved in the delivery and maintenance of this family of platforms.", 
            "title": "Audience"
        }, 
        {
            "location": "/index.html#differences-from-wiki", 
            "text": "This documentation reflects the state of play today.  If it doesn't its a bug.  This documentation is subject to the same release lifecycle as the software systems it describes.  Unlike Wiki usage, there is no attempt to capture history, collaborative input or discuss future possibilities.", 
            "title": "Differences from Wiki"
        }, 
        {
            "location": "/operation/building_a_stack/index.html", 
            "text": "Building a stack\n\n\nHow to build a full platform stack in AWS.\n\n\nAWS Access\n\n\nThere are four seperate AWS accounts or subscriptions.  Access is via the corresponding AWS console logins.\n\n\n\n\nDEV\n - for development environments, continuous integration\n\n\nTEST\n - for QA; operational acceptance and test environments\n\n\nPROD\n - for production instances\n\n\nGRN\n - for \ngreenhouse\n infrastructure; build servers, repos, LDAP etc\n\n\n\n\nConsole Login\n\n\nA member of the A\nE operational team with AWS administrative access will have first to provide you with a login to the account you need through AWS's Identity Access \n Management (IAM) access control and user management.  The username will be your Hogarthww email address.\n\n\nEC2 SSH\n\n\nWhen EC2 servers are built in AWS, access to them is governed by an SSH identity.  For this reason it is necessary to upload your SSH public key.  By convention the namimg scheme for these key pairs is \naws-\n followed by your firstname and lastname without spaces; \naws-arthurcrawford\n for example.\n\n\nWhen you access any EC2 servers built as part of a platform stack, you will use this SSH identity.\n\n\nSSH / LDAP Access Control\n\n\nAccess to resources within the AWS account is goverened by LDAP.  The way this works is that when you present your SSH identity to try and establish a connection, the identity is first validated as having suitable authority by querying against an LDAP server.\n\n\nThe underlying mechanism used to integrate SSH and LDAP in this way is \nSSSD\n.  A master LDAP server is hosted on the Greenhouse account and federates slave servers on all of the other accounts; DEV, TEST and PROD.  In this way, SSH access to VMs can be switched on and off in a single place at the master LDAP server.\n\n\nFor this reason, you must ensure also that the A\nE team have registered your SSH identity with the Greenhouse master LDAP server and that sufficient authority has been granted to you to access resources in the AWS account you need.  By convention the LDAP username format is \nfirstnamelastname\n - i.e. the latter portion of the AWS key pair name; \narthurcrawford\n in the example used here.\n\n\nThe Bastion\n\n\nIn general, the security groups and firewall settings prevent you from accessing EC2 VMs you have built from any external host.  For this purpose there is a special \nbastion\n host on each account which you must access first - for example:\n\n\n$ ssh -A arthurcrawford@bastion.eu-west-1.zonza-dev.aws.zonza.tv.\n\n\n\n[Tip: use \n-A\n flag to forward SSH identities to the endpoint SSH agent; required for subsequent jumps]\n\n\nThe Control box\n\n\nYou can't really do much from the Bastion host.  You need to first jump from the Bastion onto the Control box in order to  build and work with any stacks.\n\n\narthurcrawford@zonza-eu-west-1-bastion01:~$ ssh -A \\\n    arthurcrawford@zonza-eu-west-1-control01.eu-west-1.zonza-dev.aws.zonza.zone\n...\narthurcrawford@aws-zonza-dev-ctrl01 $\n\n\n\n[\nTip\n: create a local SSH config to perform these jumps in one step using \nProxyCommand\n]\n\n\nAWS client\n\n\nThe provisioning and orchestration scripts used to build a stack call the AWS CLI.  The CLI accesses the AWS control API using access keys.  We assume here that the client host is the control box.\n\n\nThe access keys must be created for the AWS user in the AWS console.  Once created and downloaded/copied, they are used to define a CLI profile which will be used in subsequent CLI commands.  The creation of access keys is managed in the IAM section of the AWS console.\n\n\nThe convention used by the provisioning scripts that use the CLI is to refer to a CLI profile called \nzonza-prd\n for the PROD account, \nzonza-test\n for the TEST account and so on.\n\n\nYou can run the following command on the client to see if there is already an AWS CLI profile set up with the necessary access keys.\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ aws configure list --profile zonza-prd\n      Name                    Value             Type    Location\n      ----                    -----             ----    --------\n   profile                zonza-prd           manual    --profile\naccess_key     ****************5RIA      config_file    ~/.aws/config\nsecret_key     ****************K1b5      config_file    ~/.aws/config\n    region                eu-west-1      config_file    ~/.aws/config\n\n\n\n\nIf it does not yet exist, you can create it as follows:\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ aws configure --profile=zonza-prd\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: eu-west-1\nDefault output format [None]: ENTER\n\n\n\n\n...supplying the access key values created previously in the AWS console.\n\n\nThe profile is written to the aws config file in \n~/.aws/config\n.\n\n\n$ cat ~/.aws/config\n[profile zonza-prd]\nregion = eu-west-1\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n\n\n\nThis profile may now be used any time you run the AWS CLI commands by specifying \n--profile=zonza-prd\n on the command line.  For example to list the contents of an \ns3\n bucket.\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ aws --profile=zonza-prd s3 ls s3://myBucket\n...\n\n\n\nThe same profile will also be used by the Zonza platform provisioning scripts.\n\n\nBuilding the S3 Asset Storage Account\n\n\nThe Zonza platform uses AWS S3 for storing assets.  As part of the operational tooling, a convenience command is available to build the necessary AWS resource using a \nCloudFormation\n template.\n\n\nThe necessary command for building the storage account stack is part of the package called \norchestrate-zonza-aws\n.\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ apt-get install orchestrate-zonza-aws\n\n\n\nThe following is an example of the command used to create a\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ orchestrate-storage-aws \\\n  --ca=/var/lib/secrets/ \\\n  --region=eu-west-1 \\\n  --product=zonza4 \\\n  --runbook=https://hogarthww.atlassian.net/wiki/display/TP/zonza-prd-zonza4-hww-stable.s3-website-eu-west-1.amazonaws.com \\\n  --ticket=https://hogarthww.atlassian.net/browse/ZPL-522 \\\n  --client=hww \\\n  --environment=stable \\\n  --setup\n\n\n\n\nThe above command creates an S3 user and corresponding bucket for access by the client \"hww\".  The runbook and ticket arguments are links into the live environmental information in the Wiki and the JIRA ticketing system controlling the change.  These two links provide traceability and provenance for the resources created as a result of the command.\n\n\nThe expected output of the S3 storage creation command looks something like this:\n\n\nSubscription: zonza-prd\nProduct: zonza4\nClient: hww\nEnvironment: stable\nOwner: arthurcrawford@zonza-eu-west-1-control01.eu-west-1.zonza-prd.aws.zonza.zone\nRunbook: https://hogarthww.atlassian.net/wiki/display/TP/zonza-prd-zonza4-hww-stable.s3-website-eu-west-1.amazonaws.com\nTicket: https://hogarthww.atlassian.net/browse/ZPL-522\nSubnet: subnet-5dadbe38\nSecurity Group: sg-f489c690\nRegion: eu-west-1\nAWS Profile: zonza-prd\nResourcegroup: storage-zonza4-hww-stable\n/var/lib/secrets/\n{\n    \nUser\n: {\n        \nUserId\n: \nAIDAJTMMMUCB25SCDXP2A\n,\n        \nPath\n: \n/\n,\n        \nArn\n: \narn:aws:iam::633734276892:user/zonza4-hww-stable-zonza-prd-storage-user\n,\n        \nUserName\n: \nzonza4-hww-stable-zonza-prd-storage-user\n,\n        \nCreateDate\n: \n2016-08-30T14:37:13Z\n\n    }\n}\nRunning cleanup code and exiting\nSetting up storage-zonza4-hww-stable...\n/tmp/tmp.VGM5yW4ZoF/zonza4-environment-template.json\n{\n    \nStackId\n: \narn:aws:cloudformation:eu-west-1:633734276892:stack/storage-zonza4-hww-stable/08b7fef0-7066-11e6-b4f8-50faeb4288d2\n\n}\nRunning cleanup code and exiting\n\n\n\n\nThe S3 bucket is created along with an associated user.  This is the system user that will upload content to the S3 bucket from the DAM application.  The storage creation script does not, however, automatically give this user the necessary permissions to write to S3 buckets.\n\n\nTo work around this limitation you must manually adjust the permissions in the IAM section of the AWS console by attaching an additional managed policy of \"AmazonS3FullAccess\" to the created user - in this example \nzonza4-hww-stable-zonza-prd-storage-user\n.\n\n\nIf you fail to complete this additional step, you will encouter permission errors later on when the application tries to upload assets.\n\n\nBuilding the EC2 Instances\n\n\nOnce the stack has somewhere to store assets you can create all the EC2 instances that constitute the rest of the stack.  There is a script for this.  The example below follows on from the previous ones and shows the building of a production stack for the client called \nhww\n.\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ orchestrate-zonza-aws \\\n  --ca=/var/lib/secrets/ \\\n  --runbook=https://hogarthww.atlassian.net/wiki/display/TP/zonza4-hww-stable-black.eu-west-1.zonza-prd.aws.zonza.zone \\\n  --ticket=https://hogarthww.atlassian.net/browse/ZPL-522 \\\n  --subnet=subnet-5dadbe38 \\\n  --securitygroup=sg-fc89c698 \\\n  --balancergroup=sg-eb89c68f \\\n  --prefix=zonza \\\n  --product=zonza4 \\\n  --puppet-r10k-url=https://github.hogarthww.com/puppet/control_zonza4.git \\\n  --sshkey=aws-arthurcrawford \\\n  --client=hww \\\n  --environment=stable \\\n  --colour=black \\\n  --size=venti \\\n  --setup\n\n\n\n\nOnce again the runbook and ticket arguments are to allow for tracking the origin, purpose and provenance of stacks in AWS.\n\n\nN.B. There is a \nbug\n in this script.  When you try and create a stack for a \nnew\n client for the first time, your user will not have sufficient privileges to write the necessary files when it gets to the stage of creating certificates using OpenSSL.  What you will see is an error something like this:\n\n\nCertificate Authority: /var/lib/secrets/\nRegion: eu-west-1\nAWS Profile: zonza-prd\nSSH Key: aws-arthurcrawford\nResourcegroup: zonza4-hww-stable-black\nGenerating a 2048 bit RSA private key\n...................................+++\n.................+++\nwriting new private key to 'ca/signing-ca/zonza-prd:hww:stable:eu-west-1:aws/private/signing-ca.key'\n-----\nUsing configuration from etc/subscription-ca.conf\nI am unable to access the ./ca/subscription-ca/zonza-prd directory\n./ca/subscription-ca/zonza-prd: Permission denied\nRunning cleanup code and exiting\n\n\n\n\nDebugging the bash script reveals the problem first occurs in the following line:\n\n\nbash /var/lib/secrets//orchestrate-issue-certificate /var/lib/secrets/ zonza-prd zonza-prd:hww:stable:eu-west-1:aws black zonza4-hww-stable-black eu-west-1.zonza-prd.aws.zonza.zone pup01 wap01 wap02 wap03 wap04 sql01 vid01 wrk01 rmq01 nfs01 trn01 zkp01 zkp02 zkp03 sol01 sol02 sol03 sol04 krb01 act01 xtc01\nGenerating a 2048 bit RSA private key\n.......+++\n...................+++\nwriting new private key to 'ca/signing-ca/zonza-prd:hww:stable:eu-west-1:aws/private/signing-ca.key'\n-----\nUsing configuration from etc/subscription-ca.conf\nI am unable to access the ./ca/subscription-ca/zonza-prd directory\n./ca/subscription-ca/zonza-prd: Permission denied\n+ cleanup_before_exit\n+ echo -e 'Running cleanup code and exiting'\nRunning cleanup code and exiting\n+ '[' -z '' ']'\n+ rm -rf /tmp/tmp.tmILEE1NwR\n\n\n\n\nThe workaround is to run this failing line as root instead of as your regular user.\n\n\narthurcrawford@zonza-eu-west-1-control01:~$ sudo su -\nroot@zonza-eu-west-1-control01:~# bash /var/lib/secrets//orchestrate-issue-certificate /var/lib/secrets/ zonza-prd zonza-prd:hww:stable:eu-west-1:aws black zonza4-hww-stable-black eu-west-1.zonza-prd.aws.zonza.zone pup01 wap01 wap02 wap03 wap04 sql01 vid01 wrk01 rmq01 nfs01 trn01 zkp01 zkp02 zkp03 sol01 sol02 sol03 sol04 krb01 act01 xtc01\n\n\n\n\nThis has the effect of generating the required certificates for the new client.  It works on the basis that if the certificates are already there it does not try to recreate them.  So now that this has been done, it will be possible to re-run the stack creation script.\n\n\nN.B. There is another \nbug\n in this script.  When you run it for the first time on an AWS control box it looks for the following file:\n\n\n/var/lib/secrets/secrets/legacy_replication_password\n\n\n\nIf the file is not found the script exits.  Although this file is not required for a completely fresh stack build, the file must be there.  The workaround is to \ntouch\n the file and re-run the orchestration script.\n\n\ntouch /var/lib/secrets/secrets/legacy_replication_password\n\n\n\nDNS\n\n\nThe DNS names for a Zonza stack are set up as in the following example.\n\n\n                               Name                                     Host      Role\n=====================================================================   ========  ==========\n                        hogarthww.zonza.tv                              Dynect    public DNS\n                                \u25bc\n                              CNAME\n                                \u25bc\n               hogarthww.zonza.tv.edgekey-staging.net.                  Akamai    edge\n                                \u25bc\n                        hogarthww_zonza.tv\n                                \u25bc\n         zonza4-hww-stable.eu-west-1.zonza-test.aws.zonza.tv            AWS       origin\n                                \u25bc\n                              CNAME\n                                \u25bc\n     zonza4-hww-stable-zpl-522.eu-west-1.zonza-prd.aws.zonza.tv.                  stack\n                                \u25bc\n                                A\n                                \u25bc\n zonza4-hww-stable-zpl-522-wape-200753658.eu-west-1.elb.amazonaws.com.            load bal\n=====================================================================   ========  ==========\n\n\n\n\n\nReading from top to bottom, first there is the public DNS name of the client's site. This is registered in the Dynect DNS record.  The canonical name (CNAME) for this is the Akamai Edge Key server.  The Edge Key Server in turn uses a property look up to map to the required origin server name.  The origin server name is the DNS name registered in AWS Route53.  The CNAME for this DNS is the name of the given stack which in turn directs to the A name (the actual host) which in this case is the Elastic Load Balancer (ELB) for the Web Applications (WAPs).\n\n\nNote, that this arrangement allows the stack CNAME to be switched if a new stack is built in AWS.  This allows DNS-level switching between stacks with limited impact at the Akamai CDN edge.  This is important because building of the CDN edge is very time consuming so we need to be able to leave the Akamai configuration essentially alone once it has been created.\n\n\nIn building a stack for the first time, the DNS activities are therefore:\n\n\n\n\nCreate public DNS record in Dynect\n\n\nCreate an Akamai \nproperty\n which maps the Dynect public DNS name to a public origin server DNS name in AWS/Route53.\n\n\nConfigure AWS/Route53 DNS to map the origin server DNS to the CNAME of the stack.\n\n\n\n\nAt some point in the future it will become necessary to rebuild the stack.  When the new stack is available, the CNAME for the origin server is switched to the new stack name.  Taking the example above, the origin is currently pointing at the stack called zpl-522 (the JIRA ticket raised for this release).  Imagine a new stack is built and made ready for ticket zpl-987.  The new DNS will look like this:\n\n\n                               Name                                     Host      Role\n=====================================================================   ========  ==========\n                        hogarthww.zonza.tv                              Dynect    public DNS\n                                \u25bc\n                              CNAME\n                                \u25bc\n                  hogarthww.zonza.tv.edgekey.net.                       Akamai    edge\n                                \u25bc\n                        hogarthww_zonza.tv\n                                \u25bc\n         zonza4-hww-stable.eu-west-1.zonza-test.aws.zonza.tv            AWS       origin\n                                \u25bc\n                              CNAME\n                                \u25bc\n     zonza4-hww-stable-zpl-987.eu-west-1.zonza-prd.aws.zonza.tv.                  stack\n                                \u25bc\n                                A\n                                \u25bc\n zonza4-hww-stable-zpl-987-wape-200753658.eu-west-1.elb.amazonaws.com.            load bal\n=====================================================================   ========  ==========\n\n\n\n\n\nThe only operator change necessary in the above configuration was to change the stack CNAME for the origin server to be the new stack (zpl-987).\n\n\n[Note that the provisioning scripts automatically create the A record for a stack, so this need not be created manually.]\n\n\nTearing Down a Stack\n\n\nIn order to fully tear down a stack it is necessary to delete the following AWS components.\n\n\n\n\nEC2 server stack\n\n\nS3 Storage stack\n\n\nS3 bucket\n\n\nS3 user", 
            "title": "Building a Stack"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#building-a-stack", 
            "text": "How to build a full platform stack in AWS.", 
            "title": "Building a stack"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#aws-access", 
            "text": "There are four seperate AWS accounts or subscriptions.  Access is via the corresponding AWS console logins.   DEV  - for development environments, continuous integration  TEST  - for QA; operational acceptance and test environments  PROD  - for production instances  GRN  - for  greenhouse  infrastructure; build servers, repos, LDAP etc", 
            "title": "AWS Access"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#console-login", 
            "text": "A member of the A E operational team with AWS administrative access will have first to provide you with a login to the account you need through AWS's Identity Access   Management (IAM) access control and user management.  The username will be your Hogarthww email address.", 
            "title": "Console Login"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#ec2-ssh", 
            "text": "When EC2 servers are built in AWS, access to them is governed by an SSH identity.  For this reason it is necessary to upload your SSH public key.  By convention the namimg scheme for these key pairs is  aws-  followed by your firstname and lastname without spaces;  aws-arthurcrawford  for example.  When you access any EC2 servers built as part of a platform stack, you will use this SSH identity.", 
            "title": "EC2 SSH"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#ssh-ldap-access-control", 
            "text": "Access to resources within the AWS account is goverened by LDAP.  The way this works is that when you present your SSH identity to try and establish a connection, the identity is first validated as having suitable authority by querying against an LDAP server.  The underlying mechanism used to integrate SSH and LDAP in this way is  SSSD .  A master LDAP server is hosted on the Greenhouse account and federates slave servers on all of the other accounts; DEV, TEST and PROD.  In this way, SSH access to VMs can be switched on and off in a single place at the master LDAP server.  For this reason, you must ensure also that the A E team have registered your SSH identity with the Greenhouse master LDAP server and that sufficient authority has been granted to you to access resources in the AWS account you need.  By convention the LDAP username format is  firstnamelastname  - i.e. the latter portion of the AWS key pair name;  arthurcrawford  in the example used here.", 
            "title": "SSH / LDAP Access Control"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#the-bastion", 
            "text": "In general, the security groups and firewall settings prevent you from accessing EC2 VMs you have built from any external host.  For this purpose there is a special  bastion  host on each account which you must access first - for example:  $ ssh -A arthurcrawford@bastion.eu-west-1.zonza-dev.aws.zonza.tv.  [Tip: use  -A  flag to forward SSH identities to the endpoint SSH agent; required for subsequent jumps]", 
            "title": "The Bastion"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#the-control-box", 
            "text": "You can't really do much from the Bastion host.  You need to first jump from the Bastion onto the Control box in order to  build and work with any stacks.  arthurcrawford@zonza-eu-west-1-bastion01:~$ ssh -A \\\n    arthurcrawford@zonza-eu-west-1-control01.eu-west-1.zonza-dev.aws.zonza.zone\n...\narthurcrawford@aws-zonza-dev-ctrl01 $  [ Tip : create a local SSH config to perform these jumps in one step using  ProxyCommand ]", 
            "title": "The Control box"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#aws-client", 
            "text": "The provisioning and orchestration scripts used to build a stack call the AWS CLI.  The CLI accesses the AWS control API using access keys.  We assume here that the client host is the control box.  The access keys must be created for the AWS user in the AWS console.  Once created and downloaded/copied, they are used to define a CLI profile which will be used in subsequent CLI commands.  The creation of access keys is managed in the IAM section of the AWS console.  The convention used by the provisioning scripts that use the CLI is to refer to a CLI profile called  zonza-prd  for the PROD account,  zonza-test  for the TEST account and so on.  You can run the following command on the client to see if there is already an AWS CLI profile set up with the necessary access keys.  arthurcrawford@zonza-eu-west-1-control01:~$ aws configure list --profile zonza-prd\n      Name                    Value             Type    Location\n      ----                    -----             ----    --------\n   profile                zonza-prd           manual    --profile\naccess_key     ****************5RIA      config_file    ~/.aws/config\nsecret_key     ****************K1b5      config_file    ~/.aws/config\n    region                eu-west-1      config_file    ~/.aws/config  If it does not yet exist, you can create it as follows:  arthurcrawford@zonza-eu-west-1-control01:~$ aws configure --profile=zonza-prd\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: eu-west-1\nDefault output format [None]: ENTER  ...supplying the access key values created previously in the AWS console.  The profile is written to the aws config file in  ~/.aws/config .  $ cat ~/.aws/config\n[profile zonza-prd]\nregion = eu-west-1\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY  This profile may now be used any time you run the AWS CLI commands by specifying  --profile=zonza-prd  on the command line.  For example to list the contents of an  s3  bucket.  arthurcrawford@zonza-eu-west-1-control01:~$ aws --profile=zonza-prd s3 ls s3://myBucket\n...  The same profile will also be used by the Zonza platform provisioning scripts.", 
            "title": "AWS client"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#building-the-s3-asset-storage-account", 
            "text": "The Zonza platform uses AWS S3 for storing assets.  As part of the operational tooling, a convenience command is available to build the necessary AWS resource using a  CloudFormation  template.  The necessary command for building the storage account stack is part of the package called  orchestrate-zonza-aws .  arthurcrawford@zonza-eu-west-1-control01:~$ apt-get install orchestrate-zonza-aws  The following is an example of the command used to create a  arthurcrawford@zonza-eu-west-1-control01:~$ orchestrate-storage-aws \\\n  --ca=/var/lib/secrets/ \\\n  --region=eu-west-1 \\\n  --product=zonza4 \\\n  --runbook=https://hogarthww.atlassian.net/wiki/display/TP/zonza-prd-zonza4-hww-stable.s3-website-eu-west-1.amazonaws.com \\\n  --ticket=https://hogarthww.atlassian.net/browse/ZPL-522 \\\n  --client=hww \\\n  --environment=stable \\\n  --setup  The above command creates an S3 user and corresponding bucket for access by the client \"hww\".  The runbook and ticket arguments are links into the live environmental information in the Wiki and the JIRA ticketing system controlling the change.  These two links provide traceability and provenance for the resources created as a result of the command.  The expected output of the S3 storage creation command looks something like this:  Subscription: zonza-prd\nProduct: zonza4\nClient: hww\nEnvironment: stable\nOwner: arthurcrawford@zonza-eu-west-1-control01.eu-west-1.zonza-prd.aws.zonza.zone\nRunbook: https://hogarthww.atlassian.net/wiki/display/TP/zonza-prd-zonza4-hww-stable.s3-website-eu-west-1.amazonaws.com\nTicket: https://hogarthww.atlassian.net/browse/ZPL-522\nSubnet: subnet-5dadbe38\nSecurity Group: sg-f489c690\nRegion: eu-west-1\nAWS Profile: zonza-prd\nResourcegroup: storage-zonza4-hww-stable\n/var/lib/secrets/\n{\n     User : {\n         UserId :  AIDAJTMMMUCB25SCDXP2A ,\n         Path :  / ,\n         Arn :  arn:aws:iam::633734276892:user/zonza4-hww-stable-zonza-prd-storage-user ,\n         UserName :  zonza4-hww-stable-zonza-prd-storage-user ,\n         CreateDate :  2016-08-30T14:37:13Z \n    }\n}\nRunning cleanup code and exiting\nSetting up storage-zonza4-hww-stable...\n/tmp/tmp.VGM5yW4ZoF/zonza4-environment-template.json\n{\n     StackId :  arn:aws:cloudformation:eu-west-1:633734276892:stack/storage-zonza4-hww-stable/08b7fef0-7066-11e6-b4f8-50faeb4288d2 \n}\nRunning cleanup code and exiting  The S3 bucket is created along with an associated user.  This is the system user that will upload content to the S3 bucket from the DAM application.  The storage creation script does not, however, automatically give this user the necessary permissions to write to S3 buckets.  To work around this limitation you must manually adjust the permissions in the IAM section of the AWS console by attaching an additional managed policy of \"AmazonS3FullAccess\" to the created user - in this example  zonza4-hww-stable-zonza-prd-storage-user .  If you fail to complete this additional step, you will encouter permission errors later on when the application tries to upload assets.", 
            "title": "Building the S3 Asset Storage Account"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#building-the-ec2-instances", 
            "text": "Once the stack has somewhere to store assets you can create all the EC2 instances that constitute the rest of the stack.  There is a script for this.  The example below follows on from the previous ones and shows the building of a production stack for the client called  hww .  arthurcrawford@zonza-eu-west-1-control01:~$ orchestrate-zonza-aws \\\n  --ca=/var/lib/secrets/ \\\n  --runbook=https://hogarthww.atlassian.net/wiki/display/TP/zonza4-hww-stable-black.eu-west-1.zonza-prd.aws.zonza.zone \\\n  --ticket=https://hogarthww.atlassian.net/browse/ZPL-522 \\\n  --subnet=subnet-5dadbe38 \\\n  --securitygroup=sg-fc89c698 \\\n  --balancergroup=sg-eb89c68f \\\n  --prefix=zonza \\\n  --product=zonza4 \\\n  --puppet-r10k-url=https://github.hogarthww.com/puppet/control_zonza4.git \\\n  --sshkey=aws-arthurcrawford \\\n  --client=hww \\\n  --environment=stable \\\n  --colour=black \\\n  --size=venti \\\n  --setup  Once again the runbook and ticket arguments are to allow for tracking the origin, purpose and provenance of stacks in AWS.  N.B. There is a  bug  in this script.  When you try and create a stack for a  new  client for the first time, your user will not have sufficient privileges to write the necessary files when it gets to the stage of creating certificates using OpenSSL.  What you will see is an error something like this:  Certificate Authority: /var/lib/secrets/\nRegion: eu-west-1\nAWS Profile: zonza-prd\nSSH Key: aws-arthurcrawford\nResourcegroup: zonza4-hww-stable-black\nGenerating a 2048 bit RSA private key\n...................................+++\n.................+++\nwriting new private key to 'ca/signing-ca/zonza-prd:hww:stable:eu-west-1:aws/private/signing-ca.key'\n-----\nUsing configuration from etc/subscription-ca.conf\nI am unable to access the ./ca/subscription-ca/zonza-prd directory\n./ca/subscription-ca/zonza-prd: Permission denied\nRunning cleanup code and exiting  Debugging the bash script reveals the problem first occurs in the following line:  bash /var/lib/secrets//orchestrate-issue-certificate /var/lib/secrets/ zonza-prd zonza-prd:hww:stable:eu-west-1:aws black zonza4-hww-stable-black eu-west-1.zonza-prd.aws.zonza.zone pup01 wap01 wap02 wap03 wap04 sql01 vid01 wrk01 rmq01 nfs01 trn01 zkp01 zkp02 zkp03 sol01 sol02 sol03 sol04 krb01 act01 xtc01\nGenerating a 2048 bit RSA private key\n.......+++\n...................+++\nwriting new private key to 'ca/signing-ca/zonza-prd:hww:stable:eu-west-1:aws/private/signing-ca.key'\n-----\nUsing configuration from etc/subscription-ca.conf\nI am unable to access the ./ca/subscription-ca/zonza-prd directory\n./ca/subscription-ca/zonza-prd: Permission denied\n+ cleanup_before_exit\n+ echo -e 'Running cleanup code and exiting'\nRunning cleanup code and exiting\n+ '[' -z '' ']'\n+ rm -rf /tmp/tmp.tmILEE1NwR  The workaround is to run this failing line as root instead of as your regular user.  arthurcrawford@zonza-eu-west-1-control01:~$ sudo su -\nroot@zonza-eu-west-1-control01:~# bash /var/lib/secrets//orchestrate-issue-certificate /var/lib/secrets/ zonza-prd zonza-prd:hww:stable:eu-west-1:aws black zonza4-hww-stable-black eu-west-1.zonza-prd.aws.zonza.zone pup01 wap01 wap02 wap03 wap04 sql01 vid01 wrk01 rmq01 nfs01 trn01 zkp01 zkp02 zkp03 sol01 sol02 sol03 sol04 krb01 act01 xtc01  This has the effect of generating the required certificates for the new client.  It works on the basis that if the certificates are already there it does not try to recreate them.  So now that this has been done, it will be possible to re-run the stack creation script.  N.B. There is another  bug  in this script.  When you run it for the first time on an AWS control box it looks for the following file:  /var/lib/secrets/secrets/legacy_replication_password  If the file is not found the script exits.  Although this file is not required for a completely fresh stack build, the file must be there.  The workaround is to  touch  the file and re-run the orchestration script.  touch /var/lib/secrets/secrets/legacy_replication_password", 
            "title": "Building the EC2 Instances"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#dns", 
            "text": "The DNS names for a Zonza stack are set up as in the following example.                                 Name                                     Host      Role\n=====================================================================   ========  ==========\n                        hogarthww.zonza.tv                              Dynect    public DNS\n                                \u25bc\n                              CNAME\n                                \u25bc\n               hogarthww.zonza.tv.edgekey-staging.net.                  Akamai    edge\n                                \u25bc\n                        hogarthww_zonza.tv\n                                \u25bc\n         zonza4-hww-stable.eu-west-1.zonza-test.aws.zonza.tv            AWS       origin\n                                \u25bc\n                              CNAME\n                                \u25bc\n     zonza4-hww-stable-zpl-522.eu-west-1.zonza-prd.aws.zonza.tv.                  stack\n                                \u25bc\n                                A\n                                \u25bc\n zonza4-hww-stable-zpl-522-wape-200753658.eu-west-1.elb.amazonaws.com.            load bal\n=====================================================================   ========  ==========  Reading from top to bottom, first there is the public DNS name of the client's site. This is registered in the Dynect DNS record.  The canonical name (CNAME) for this is the Akamai Edge Key server.  The Edge Key Server in turn uses a property look up to map to the required origin server name.  The origin server name is the DNS name registered in AWS Route53.  The CNAME for this DNS is the name of the given stack which in turn directs to the A name (the actual host) which in this case is the Elastic Load Balancer (ELB) for the Web Applications (WAPs).  Note, that this arrangement allows the stack CNAME to be switched if a new stack is built in AWS.  This allows DNS-level switching between stacks with limited impact at the Akamai CDN edge.  This is important because building of the CDN edge is very time consuming so we need to be able to leave the Akamai configuration essentially alone once it has been created.  In building a stack for the first time, the DNS activities are therefore:   Create public DNS record in Dynect  Create an Akamai  property  which maps the Dynect public DNS name to a public origin server DNS name in AWS/Route53.  Configure AWS/Route53 DNS to map the origin server DNS to the CNAME of the stack.   At some point in the future it will become necessary to rebuild the stack.  When the new stack is available, the CNAME for the origin server is switched to the new stack name.  Taking the example above, the origin is currently pointing at the stack called zpl-522 (the JIRA ticket raised for this release).  Imagine a new stack is built and made ready for ticket zpl-987.  The new DNS will look like this:                                 Name                                     Host      Role\n=====================================================================   ========  ==========\n                        hogarthww.zonza.tv                              Dynect    public DNS\n                                \u25bc\n                              CNAME\n                                \u25bc\n                  hogarthww.zonza.tv.edgekey.net.                       Akamai    edge\n                                \u25bc\n                        hogarthww_zonza.tv\n                                \u25bc\n         zonza4-hww-stable.eu-west-1.zonza-test.aws.zonza.tv            AWS       origin\n                                \u25bc\n                              CNAME\n                                \u25bc\n     zonza4-hww-stable-zpl-987.eu-west-1.zonza-prd.aws.zonza.tv.                  stack\n                                \u25bc\n                                A\n                                \u25bc\n zonza4-hww-stable-zpl-987-wape-200753658.eu-west-1.elb.amazonaws.com.            load bal\n=====================================================================   ========  ==========  The only operator change necessary in the above configuration was to change the stack CNAME for the origin server to be the new stack (zpl-987).  [Note that the provisioning scripts automatically create the A record for a stack, so this need not be created manually.]", 
            "title": "DNS"
        }, 
        {
            "location": "/operation/building_a_stack/index.html#tearing-down-a-stack", 
            "text": "In order to fully tear down a stack it is necessary to delete the following AWS components.   EC2 server stack  S3 Storage stack  S3 bucket  S3 user", 
            "title": "Tearing Down a Stack"
        }, 
        {
            "location": "/operation/retired_instance/index.html", 
            "text": "AWS Retired Instance\n\n\nAWS will notifiy of instances that will be retired.\n\n\nThe reasons for retirement include redundant hardware.  AWS will stop these instances according to the notified schedule.\n\n\nSee the \nfull details\n of what happens when instances are retired in the AWS documentation.\n\n\nIn summary, if the root device type of the instance is EBS, the following instructions apply.\n\n\n\n\nWait for the scheduled retirement date - when the instance is stopped - or stop the instance yourself before the retirement date. You can start the instance again at any time. For more information about stopping and starting your instance, and what to expect when your instance is stopped, such as the effect on public, private and Elastic IP addresses associated with your instance, see Stop and Start Your Instance.", 
            "title": "AWS Retired Instance"
        }, 
        {
            "location": "/operation/retired_instance/index.html#aws-retired-instance", 
            "text": "AWS will notifiy of instances that will be retired.  The reasons for retirement include redundant hardware.  AWS will stop these instances according to the notified schedule.  See the  full details  of what happens when instances are retired in the AWS documentation.  In summary, if the root device type of the instance is EBS, the following instructions apply.   Wait for the scheduled retirement date - when the instance is stopped - or stop the instance yourself before the retirement date. You can start the instance again at any time. For more information about stopping and starting your instance, and what to expect when your instance is stopped, such as the effect on public, private and Elastic IP addresses associated with your instance, see Stop and Start Your Instance.", 
            "title": "AWS Retired Instance"
        }, 
        {
            "location": "/operation/troubleshooting/index.html", 
            "text": "Troubleshooting\n\n\nProblem: WAP(s) fail to mount NFS shares on restart\n\n\nSymptom\n\n\nIntermittent errors in ingest processing.\n\n\nCause\n\n\nThis problem is caused by a WAP failing to mount NFS shares following a restart.\nSome ingests may continue to work, however, if they are routed through fully functioning WAPs by the load balancer.\n\n\nOne scenario in which this can occur is when an EC2 instance is retired.  In such a case it is necessary to start the instance again, which moves it onto new hardware and re-attaches to the original Elastic Block Storage (EBS) device.  The problem can occur at this stage when the NFS mounts may not be correctly re-established.\n\n\nIssue Tracking\n\n\nThis has been reported as a bug - JIRA Issue ().\n\n\nA proper fix to this issue is to have the system automatically correct this issue itself.  Also, a WAP, while in such a failed, or unready state should not be available for processing transactions.\n\n\nDiagnostics\n\n\nTo diagnose this problem you can do one of the following:\n\n\narthurcrawford@zonza4-art-testing-po-wap01:~$ mount\narthurcrawford@zonza4-art-testing-po-wap01:~$ cat /etc/mtab\n\n\n\nIn either case, a healthy WAP will show something like the following mount points in the list.\n\n\n...\nzonza4-art-testing-po-nfs01.eu-west-1.zonza-dev.aws.zonza.zone:/share/wappshare /opt/hogarth/shared nfs4 rw,addr=172.29.194.205,clientaddr=172.29.193.163,_netdev 0 0\nzonza4-art-testing-po-nfs01.eu-west-1.zonza-dev.aws.zonza.zone:/share/ingest /opt/vidispine/ingest nfs4 rw,addr=172.29.194.205,clientaddr=172.29.193.163,_netdev 0 0\n...\n\n\n\n\nThe important things to note are the following mounts from the NFS server.\n\n\n/share/wappshare /opt/hogarth/shared\n/share/ingest /opt/vidispine/ingest\n\n\n\nIf you do not see the mount points there is a problem.  These are shared between the cluster of WAPs.\n\n\nFurther diagnose the issue by checking the log for the \nmountall\n process run at startup by the \nupstart\n init process.\n\n\n$ sudo cat /var/log/upstart/mountall.log\nmountall: Disconnected from Plymouth\nmount.nfs: Connection timed out\nmount.nfs: Connection timed out\nmountall: mount /opt/hogarth/shared [992] terminated with status 32\nmountall: mount /opt/vidispine/ingest [994] terminated with status 32\n\n\n\n\nNote in the above log how the \nmount\n timed out and the mount points failed indicating the problem.\n\n\nYou can also check the status of the \nmountall\n \nupstart\n job using one of the following commands:\n\n\n$ initctl status mountall\nmountall stop/waiting\n\n$ sudo service mountall status\nmountall stop/waiting\n\n\n\nThe \nmountall\n process should not be running - it should be in the stop/waiting state as shown above.  If it is running, you will see a process ID instead - that indicates a problem occurred and the \nmountall\n process did not complete successfully.\n\n\nOperational Workaround\n\n\nRestart the \nmountall\n job on the failing WAP.\n\n\nzpl-522-wap04:~$ sudo service mountall restart\nmountall stop/waiting\nmountall stop/waiting\n\n\n\nThat should restore the mounts correctly (check in \n/etc/mtab\n).  You should also check again the status of the \nmountall\n process.  Validate that it is now in the \nstop/waiting\n state.\n\n\n$ sudo service mountall status\nmountall stop/waiting\n\n\n\nProblem: EC2 instances stop when AWS retires their hardware\n\n\nSymptom\n\n\nA number of possible types of degradation of service.\n\n\nCause\n\n\nAn EC2 instance or instances have been stopped by AWS.\n\n\nAWS will notifiy of instances that will be retired.\n\n\nThe reasons for retirement include redundant hardware.  AWS will stop these instances according to the notified schedule.\n\n\nSee the \nfull details\n of what happens when instances are retired in the AWS documentation.\n\n\nIn summary, if the root device type of the instance is EBS, the following instructions apply.\n\n\n\n\nWait for the scheduled retirement date - when the instance is stopped - or stop the instance yourself before the retirement date. You can start the instance again at any time. For more information about stopping and starting your instance, and what to expect when your instance is stopped, such as the effect on public, private and Elastic IP addresses associated with your instance, see Stop and Start Your Instance.\n\n\n\n\nIssue Tracking\n\n\nTracked by JIRA Issue ().\n\n\nThe desirable fix is that stopped instances are detected and restarted automatically.\n\n\nDiagnostics\n\n\nAWS console\n\n\nAWS CLI\n\n\nOperational Workaround\n\n\nRestart the instance\n\n\n\n\nAWS console\n\n\nAWS CLI", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#problem-waps-fail-to-mount-nfs-shares-on-restart", 
            "text": "", 
            "title": "Problem: WAP(s) fail to mount NFS shares on restart"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#symptom", 
            "text": "Intermittent errors in ingest processing.", 
            "title": "Symptom"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#cause", 
            "text": "This problem is caused by a WAP failing to mount NFS shares following a restart.\nSome ingests may continue to work, however, if they are routed through fully functioning WAPs by the load balancer.  One scenario in which this can occur is when an EC2 instance is retired.  In such a case it is necessary to start the instance again, which moves it onto new hardware and re-attaches to the original Elastic Block Storage (EBS) device.  The problem can occur at this stage when the NFS mounts may not be correctly re-established.", 
            "title": "Cause"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#issue-tracking", 
            "text": "This has been reported as a bug - JIRA Issue ().  A proper fix to this issue is to have the system automatically correct this issue itself.  Also, a WAP, while in such a failed, or unready state should not be available for processing transactions.", 
            "title": "Issue Tracking"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#diagnostics", 
            "text": "To diagnose this problem you can do one of the following:  arthurcrawford@zonza4-art-testing-po-wap01:~$ mount\narthurcrawford@zonza4-art-testing-po-wap01:~$ cat /etc/mtab  In either case, a healthy WAP will show something like the following mount points in the list.  ...\nzonza4-art-testing-po-nfs01.eu-west-1.zonza-dev.aws.zonza.zone:/share/wappshare /opt/hogarth/shared nfs4 rw,addr=172.29.194.205,clientaddr=172.29.193.163,_netdev 0 0\nzonza4-art-testing-po-nfs01.eu-west-1.zonza-dev.aws.zonza.zone:/share/ingest /opt/vidispine/ingest nfs4 rw,addr=172.29.194.205,clientaddr=172.29.193.163,_netdev 0 0\n...  The important things to note are the following mounts from the NFS server.  /share/wappshare /opt/hogarth/shared\n/share/ingest /opt/vidispine/ingest  If you do not see the mount points there is a problem.  These are shared between the cluster of WAPs.  Further diagnose the issue by checking the log for the  mountall  process run at startup by the  upstart  init process.  $ sudo cat /var/log/upstart/mountall.log\nmountall: Disconnected from Plymouth\nmount.nfs: Connection timed out\nmount.nfs: Connection timed out\nmountall: mount /opt/hogarth/shared [992] terminated with status 32\nmountall: mount /opt/vidispine/ingest [994] terminated with status 32  Note in the above log how the  mount  timed out and the mount points failed indicating the problem.  You can also check the status of the  mountall   upstart  job using one of the following commands:  $ initctl status mountall\nmountall stop/waiting\n\n$ sudo service mountall status\nmountall stop/waiting  The  mountall  process should not be running - it should be in the stop/waiting state as shown above.  If it is running, you will see a process ID instead - that indicates a problem occurred and the  mountall  process did not complete successfully.", 
            "title": "Diagnostics"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#operational-workaround", 
            "text": "Restart the  mountall  job on the failing WAP.  zpl-522-wap04:~$ sudo service mountall restart\nmountall stop/waiting\nmountall stop/waiting  That should restore the mounts correctly (check in  /etc/mtab ).  You should also check again the status of the  mountall  process.  Validate that it is now in the  stop/waiting  state.  $ sudo service mountall status\nmountall stop/waiting", 
            "title": "Operational Workaround"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#problem-ec2-instances-stop-when-aws-retires-their-hardware", 
            "text": "", 
            "title": "Problem: EC2 instances stop when AWS retires their hardware"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#symptom_1", 
            "text": "A number of possible types of degradation of service.", 
            "title": "Symptom"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#cause_1", 
            "text": "An EC2 instance or instances have been stopped by AWS.  AWS will notifiy of instances that will be retired.  The reasons for retirement include redundant hardware.  AWS will stop these instances according to the notified schedule.  See the  full details  of what happens when instances are retired in the AWS documentation.  In summary, if the root device type of the instance is EBS, the following instructions apply.   Wait for the scheduled retirement date - when the instance is stopped - or stop the instance yourself before the retirement date. You can start the instance again at any time. For more information about stopping and starting your instance, and what to expect when your instance is stopped, such as the effect on public, private and Elastic IP addresses associated with your instance, see Stop and Start Your Instance.", 
            "title": "Cause"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#issue-tracking_1", 
            "text": "Tracked by JIRA Issue ().  The desirable fix is that stopped instances are detected and restarted automatically.", 
            "title": "Issue Tracking"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#diagnostics_1", 
            "text": "AWS console  AWS CLI", 
            "title": "Diagnostics"
        }, 
        {
            "location": "/operation/troubleshooting/index.html#operational-workaround_1", 
            "text": "Restart the instance   AWS console  AWS CLI", 
            "title": "Operational Workaround"
        }, 
        {
            "location": "/operation/tips/index.html", 
            "text": "Tips\n\n\nSSH Jump to Control box via Bastion in one step\n\n\nHere's one way - not perfect but it works.\n\n\nAdd something like this to your \n~/.ssh/config\n file.\n\n\n...\nHost bastion.eu-west-1.zonza-*\n  HostName %h.aws.zonza.tv\n  User arthurcrawford\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/aws.hogarthww.com/id_rsa\n  IdentitiesOnly yes\n  ForwardAgent yes\nHost zonza-*\n  ProxyCommand ssh -q bastion.eu-west-1.%h nc zonza-eu-west-1-control01.eu-west-1.%h.aws.zonza.zone 22\n  User arthurcrawford\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/aws.hogarthww.com/id_rsa\n  IdentitiesOnly yes\n  ForwardAgent yes\n...\n\n\n\n\n[Note: later versions of OpenSSH \n 7.3 allow this to be done more directly using the \nProxyJump\n command.]\n\n\n[Tip: as shown here, it's a good idea to use a separate SSH identity for the purpose of accessing AWS accounts - it's better not to use the same identity for accessing everything in case this private key is compromised.]\n\n\nTo complement the above SSH config, provide a handy set of aliases in your \nbash\n shell profile.\n\n\nalias zpl-prd='ssh zonza-prd'\nalias zpl-test='ssh zonza-test'\nalias zpl-dev='ssh zonza-dev'\n\n\n\n\nAs an example, typing \nzpl-prd\n in the shell is an alias for \nssh zonza-prd\n which is matched by the lower block of SSH config. This establishes the SSH connection to the matching bastion host by expanding \n%h\n to \nzonza-prd\n to complete the fully qualified name of the appropriate bastion host for PROD.  The connection is then proxied to the appropriate control box using \nnc\n (\nnetcat\n), again using the \n%h\n placeholder to complete the fully qualified name of the PROD control box.\n\n\nExample:\n\n\nart@MyMac:~/$ zpl-prd\n...\nLast login: Tue Aug 30 16:00:25 2016 from zonza-eu-west-1-bastion01.eu-west-1.zonza-prd.aws.zonza.zone\narthurcrawford@zonza-eu-west-1-control01:~$", 
            "title": "Tips"
        }, 
        {
            "location": "/operation/tips/index.html#tips", 
            "text": "", 
            "title": "Tips"
        }, 
        {
            "location": "/operation/tips/index.html#ssh-jump-to-control-box-via-bastion-in-one-step", 
            "text": "Here's one way - not perfect but it works.  Add something like this to your  ~/.ssh/config  file.  ...\nHost bastion.eu-west-1.zonza-*\n  HostName %h.aws.zonza.tv\n  User arthurcrawford\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/aws.hogarthww.com/id_rsa\n  IdentitiesOnly yes\n  ForwardAgent yes\nHost zonza-*\n  ProxyCommand ssh -q bastion.eu-west-1.%h nc zonza-eu-west-1-control01.eu-west-1.%h.aws.zonza.zone 22\n  User arthurcrawford\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/aws.hogarthww.com/id_rsa\n  IdentitiesOnly yes\n  ForwardAgent yes\n...  [Note: later versions of OpenSSH   7.3 allow this to be done more directly using the  ProxyJump  command.]  [Tip: as shown here, it's a good idea to use a separate SSH identity for the purpose of accessing AWS accounts - it's better not to use the same identity for accessing everything in case this private key is compromised.]  To complement the above SSH config, provide a handy set of aliases in your  bash  shell profile.  alias zpl-prd='ssh zonza-prd'\nalias zpl-test='ssh zonza-test'\nalias zpl-dev='ssh zonza-dev'  As an example, typing  zpl-prd  in the shell is an alias for  ssh zonza-prd  which is matched by the lower block of SSH config. This establishes the SSH connection to the matching bastion host by expanding  %h  to  zonza-prd  to complete the fully qualified name of the appropriate bastion host for PROD.  The connection is then proxied to the appropriate control box using  nc  ( netcat ), again using the  %h  placeholder to complete the fully qualified name of the PROD control box.  Example:  art@MyMac:~/$ zpl-prd\n...\nLast login: Tue Aug 30 16:00:25 2016 from zonza-eu-west-1-bastion01.eu-west-1.zonza-prd.aws.zonza.zone\narthurcrawford@zonza-eu-west-1-control01:~$", 
            "title": "SSH Jump to Control box via Bastion in one step"
        }
    ]
}